{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Text Classification\n",
    "\n",
    "# The text classification workflow begins by cleaning and preparing the corpus out of the dataset. Then this corpus is represented by any of the different text representation methods which are hen followed by modeling.\n"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "source": [
    "# - Example text classification dataset   \n",
    "# use the data from tweets,the task is to predict which tweets are about real disasters and which ones are not."
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   id keyword  ...                                               text target\n0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n2   5     NaN  ...  All residents asked to &#39;shelter in place&#39; are ...      1\n\n[3 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "tweet = pd.read_csv(r'data/tweets/train.csv')\n",
    "test = pd.read_csv(r'data/tweets/test.csv')\n",
    "\n",
    "tweet.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 7613 rows and 5 columns in train\nThere are 3263 rows and 4 columns in test\n"
    }
   ],
   "source": [
    "# We will only consider the tweets to predict the target.\n",
    "print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\n",
    "print('There are {} rows and {} columns in test'.format(test.shape[0],test.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text data preparation\n",
    "\n",
    "# Tokenize: the process by which sentences are converted to a list of tokens or words.\n",
    "# Remove stopwords: drop words like ‘a’ or ‘the’\n",
    "# Lemmatize: reduce the inflectional forms of each word into a common base or root (“studies”, “studying” -> “study”).\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def preprocess_texts(df, col):\n",
    "    new_corpus = []\n",
    "\n",
    "    lem = WordNetLemmatizer()\n",
    "    for text in df[col]:\n",
    "        words = [w for w in word_tokenize(text)  if (w not in stopwords)]\n",
    "        words = [lem.lemmatize(w) for w in words]\n",
    "        new_corpus.append(words)\n",
    "    \n",
    "    return new_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Representation\n",
    "\n",
    "# Text cannot be used directly as input to a machine learning model but needs to be represented in the numeric format first. This is known as text representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{&#39;she&#39;: 4, &#39;sells&#39;: 3, &#39;seashells&#39;: 1, &#39;in&#39;: 0, &#39;the&#39;: 5, &#39;seashore&#39;: 2}\n(1, 6)\n&lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt;\n[[1 1 1 1 1 1]]\n"
    }
   ],
   "source": [
    "# 1. Countvectorizer\n",
    "# Countvectorizer provides an easy method to vectorize and represent a collection of text documents. It tokenizes the input text and builds a vocabulary of known words and then represents the documents using this vocabulary.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = ['She sells seashells in the seashore']\n",
    "vectorizer = CountVectorizer()  # create the transform\n",
    "vectorizer.fit(text)  # tokenize and build vocab\n",
    "\n",
    "print(vectorizer.vocabulary_)  \n",
    "\n",
    "vector = vectorizer.transform(text)  # encode document\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[[1 1 1 0 0 1]]\n"
    }
   ],
   "source": [
    "# the Coutvectorizer has built a vocabulary out of the given text and then represented the words using a numpy sparse matrix. We can try and transfer another text using this vocabulary and observe the output to get a better understanding.\n",
    "vector = vectorizer.transform(['I sell seashells in the seashore'])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{&#39;she&#39;: 5, &#39;sells&#39;: 4, &#39;seashells&#39;: 2, &#39;by&#39;: 0, &#39;the&#39;: 6, &#39;seashore&#39;: 3, &#39;sea&#39;: 1}\n[1.69314718 1.69314718 1.69314718 1.28768207 1.69314718 1.69314718\n 1.        ]\n(1, 7)\n[[0.45050407 0.         0.45050407 0.34261996 0.45050407 0.45050407\n  0.26607496]]\n"
    }
   ],
   "source": [
    "# 2. TfidfVectorizer\n",
    "# One issue with Countvectorizer is that common words like “the” will appear many times (unless you remove them at the preprocessing stage) and these words are not actually important. One popular alternative is Tfidfvectorizer. It is an acronym for Term frequency-inverse document frequency.\n",
    "\n",
    "# Term Frequency: This summarizes how often a given word appears within a document.\n",
    "# Inverse Document Frequency: This downscales words that appear a lot across documents.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [\"She sells seashells by the seashore\",\"The sea.\",\"The seashore\"]\n",
    "vectorizer = TfidfVectorizer()  # create the transform\n",
    "vectorizer.fit(text)  # tokenize and build vocab\n",
    "\n",
    "print(vectorizer.vocabulary_)\n",
    "print(vectorizer.idf_)\n",
    "\n",
    "vector = vectorizer.transform([text[0]])  # encode document\n",
    "print(vector.shape)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vocabulary again consists of 6 words and the inverse document frequency is calculated for each word, assigning the lowest score to “the” which occurred 4 times.\n",
    "\n",
    "# Then the scores are normalized between 0 and 1 and this text representation can be used as input into any machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. word2vec\n",
    "# The big issue with the above approaches is that the context of the word is lost when representing it. Word embeddings are learned by understanding the context in which the word occurs. Specifically, it looks at co-occurring words.The basic idea of word embedding is words that occur in similar context tend to be closer to each other in vector space. \n",
    "\n",
    "# Word2vec is composed of two different models:\n",
    "\n",
    "# Continuous Bag of Words (CBOW) model can be thought of as learning word embeddings by training a model to predict a word given its context.\n",
    "# Skip-Gram model is the opposite, learning word embeddings by training a model to predict context given a word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pre-trained word vectors instead of training vectors from our corpus. \n",
    "\n",
    "from gensim.models import keyedvectors\n",
    "\n",
    "def load_word2vec(pretrained_path):\n",
    "    word2vec_dict = keyedvectors.load(pretrained_path, binary=True, unicode_errors='ignore')\n",
    "    embeddings_index = dict()\n",
    "    for word in word2vec_dict.wv.vocab:\n",
    "        embeddings_index[word] = word2vec_dict.word_vec(word)\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model = load_word2vec('dataset')\n",
    "# print(w2v_model['London'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the word is represented using a 300-dimensional vector. So every word in your corpus can be represented like this and this embedding matrix is used to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. FastText\n",
    "# It supports both Continuous Bag of Words and Skip-Gram models. The main difference between previous models and FastText is that it breaks the word in several n-grams.\n",
    "\n",
    "# Let’s take the word orange for example.\n",
    "\n",
    "# The trigrams of word orange are,org,ran,ang,nge(ignoring the starting and ending boundaries of the word).\n",
    "\n",
    "# The word embedding vector (text representation)for orange will be the sum of these n-grams. Rare words or typos can now be properly represented since it is highly likely that some of their n-grams also appears in other words.\n",
    "\n",
    "# For example, for a word like stupedofantabulouslyfantastic, which might never have been in any corpus, gensim might return any two of the following solutions: a zero vector or a random vector with low magnitude.\n",
    "\n",
    "# FastText, however, can produce better vectors by breaking the word into chunks and using the vectors for those chunks to create a final vector for the word. In this particular case, the final vector might be closer to the vectors of fantastic and fantabulous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "from gensim.models import FastText\n",
    "\n",
    "def load_fasttext(fpath):\n",
    "    print('loading fasttext word embeddings...')\n",
    "    embeddings_index = {}\n",
    "    with open(fpath, encoding='utf-8') as fr:\n",
    "        for line in tqdm(fr):\n",
    "            values = line.strip().rsplit(' ')\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "        \n",
    "        print(f'found {len(embeddings_index)}s word vectors.')\n",
    "    \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "824it [00:00, 8237.04it/s]loading fasttext word embeddings...\n999995it [01:17, 12984.39it/s]found 999995s word vectors.\n(300,)\n\n"
    }
   ],
   "source": [
    "w2v_fasttext = load_fasttext(r'/Users/liuzhi/models/w2v/wiki-news-300d-1M-subword.vec')\n",
    "print(w2v_fasttext['London'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. GloVe (Global vectors for word representation)\n",
    "\n",
    "# GloVe主要的思想是基于词的共现矩阵，类似于Word2Vec，但略有差别。\n",
    "# Word2vec relies only on local information of language.\n",
    "# GloVe captures both global statistics and local statistics of a corpus, in order to come up with word vectors.\n",
    "# Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j.\n",
    "'''\n",
    "Consider the entity\n",
    "P_ik/P_jk where P_ik = X_ik/X_i\n",
    "Here P_ik denotes the probability of seeing word i and k together, which is computed by dividing the number of times i and k appeared together (X_ik) by the total number of times word i appeared in the corpus (X_i).\n",
    "You can see that given two words, i.e. ice and steam, if the third word k (also called the “probe word”),\n",
    "is very similar to ice but irrelevant to steam (e.g. k=solid), P_ik/P_jk will be very high (>1),\n",
    "is very similar to steam but irrelevant to ice (e.g. k=gas), P_ik/P_jk will be very small (<1),\n",
    "is related or unrelated to either words, then P_ik/P_jk will be close to 1\n",
    "So, if we can find a way to incorporate P_ik/P_jk to computing word vectors we will be achieving the goal of using global statistics when learning word vectors.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(fpath):\n",
    "    print('loading glove word embeddings...')\n",
    "    embeddings_dict = {}\n",
    "    with open(fpath, encoding='utf-8') as fr:\n",
    "        for line in fr:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vectors = np.asarray(values[1:], 'float32')\n",
    "            embeddings_dict[word] = vectors\n",
    "    \n",
    "    print(f'found {len(embeddings_dict)}s word vectors.')\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_glove = load_glove('dataset')\n",
    "# print(w2v_glove['London'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Universal Sentence Encoding\n",
    "\n",
    "# Sometimes we need to explore sentence level operations. These encoders are called sentence encoders.\n",
    "# A good sentence encoder is expected to encode sentences in such a way that the vectors of similar sentences have a minimal distance between them in the vector space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# import tensorflow_hub as hub\n",
    "# module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\"\n",
    "# # Import the Universal Sentence Encoder's TF Hub module\n",
    "# embed = hub.Module(module_url)\n",
    "\n",
    "# # Compute a representation for each message, showing various lengths supported.\n",
    "# messages = [\"That band rocks!\", \"That song is really cool.\"]\n",
    "\n",
    "# with tf.Session() as session:\n",
    "#   session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "#   message_embeddings = session.run(embed(messages))"
   ]
  }
 ]
}