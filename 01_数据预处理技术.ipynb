{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 均值移除(Mean removal)\n",
    "把每个特征的平均值移除，以保证特征均值为0（即标准化处理），这样做可以消除特征彼此间的偏差。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(3, 4)\nMean=[ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]\nMean2=[ 5.55111512e-17 -1.11022302e-16 -7.40148683e-17 -7.40148683e-17]\nStandardized:\n[[ 1.33630621 -1.40451644  1.29110641 -0.86687558]\n [-1.06904497  0.84543708 -0.14577008  1.40111286]\n [-0.26726124  0.55907936 -1.14533633 -0.53423728]]\nSTD=[1. 1. 1. 1.]\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "data=np.array([[3, -1.5, 2, -5.4],\n",
    "               [0, 4,-0.3,2.1],\n",
    "               [1, 3.3, -1.9, -4.3]]) # 原始数据矩阵 shape=(3,4)\n",
    "\n",
    "data_standardized = preprocessing.scale(data)\n",
    "\n",
    "print(data_standardized.shape)\n",
    "print(f\"Mean={data_standardized.mean(axis=0)}\")\n",
    "print(f\"Mean2={np.mean(data_standardized, axis=0)}\")\n",
    "print(f\"Standardized:\")\n",
    "print(data_standardized)\n",
    "print(f\"STD={np.std(data_standardized, axis=0)}\")"
   ]
  },
  {
   "source": [
    "- 值移除之后的矩阵每一列的均值约为0，而std为1。这样做的目的是确保每一个特征列的数值都在类似的数据范围之间，防止某一个特征列数据天然的数值太大而一家独大。\n",
    "- 调用preprocessing模块中成熟的scale方法来对一个numpy 矩阵进行均值移除。\n",
    "- 求一个numpy矩阵的平均值（或std，min,max等）至少有两种方法"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 范围缩放(Scaling)\n",
    "数据点中每个特征列的数值范围可能变化很大，因此，有时需要将特征列的数值范围缩放到合理的大小。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "scaled matrix:\n[[1.         0.         1.         0.        ]\n [0.         1.         0.41025641 1.        ]\n [0.33333333 0.87272727 0.         0.14666667]]\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "data=np.array([[3, -1.5, 2, -5.4],\n",
    "               [0, 4,-0.3,2.1],\n",
    "               [1, 3.3, -1.9, -4.3]]) # 原始数据矩阵 shape=(3,4)\n",
    "\n",
    "data_scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)) # 缩放至 (0, 1) 之间\n",
    "data_scaled = data_scaler.fit_transform(data)\n",
    "\n",
    "print(f'scaled matrix:')\n",
    "print(data_scaled)"
   ]
  },
  {
   "source": [
    "### 归一化(Normalization)\n",
    "用于需要对特征向量的值进行调整时，以保证每个特征向量的值都缩放到相同的数值范围。   \n",
    "机器学习中最常用的归一化形式就是将特征向量调整为L1范数，使特征向量的数值之和为1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "L1 normalized matrix:\n[[ 0.25210084 -0.12605042  0.16806723 -0.45378151]\n [ 0.          0.625      -0.046875    0.328125  ]\n [ 0.0952381   0.31428571 -0.18095238 -0.40952381]]\nsum of matrix: 0.5656337535014005\nL2 normalized matrix:\n[[ 0.45017448 -0.22508724  0.30011632 -0.81031406]\n [ 0.          0.88345221 -0.06625892  0.46381241]\n [ 0.17152381  0.56602858 -0.32589524 -0.73755239]]\nsum of matrix: 0.6699999596689536\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "data=np.array([[3, -1.5, 2, -5.4],\n",
    "               [0, 4,-0.3,2.1],\n",
    "               [1, 3.3, -1.9, -4.3]]) # 原始数据矩阵 shape=(3,4)\n",
    "\n",
    "data_l1_normalized = preprocessing.normalize(data, norm='l1')\n",
    "print(f'L1 normalized matrix:')\n",
    "print(data_l1_normalized)\n",
    "print(f'sum of matrix: {np.sum(data_l1_normalized)}')\n",
    "\n",
    "data_l2_normalized = preprocessing.normalize(data)  # 默认 norm='l2'\n",
    "print(f'L2 normalized matrix:')\n",
    "print(data_l2_normalized)\n",
    "print(f'sum of matrix: {np.sum(data_l2_normalized)}')"
   ]
  },
  {
   "source": [
    "- Normaliztion之后所有的特征向量的值都缩放到同一个数值范围，可以确保数据点没有因为特征的基本性质而产生的较大差异，即确保所有数据点都处于同一个数据量，提高不同特征数据的可比性。\n",
    "- 注意和均值移除的区别：均值移除是对每一个特征列都缩放到类似的数值范围，每一个特征列的均值为0，而Normalization是将全局所有数值都缩放到同一个数值范围。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 二值化(Binarization)\n",
    "二值化用于将数值特征向量转换为布尔类型向量。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "binarized matix:\n[[1. 0. 1. 0.]\n [0. 1. 0. 1.]\n [0. 1. 0. 0.]]\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "data=np.array([[3, -1.5, 2, -5.4],\n",
    "               [0, 4,-0.3,2.1],\n",
    "               [1, 3.3, -1.9, -4.3]]) # 原始数据矩阵 shape=(3,4)\n",
    "\n",
    "data_binarized = preprocessing.Binarizer(threshold=1.4).transform(data)\n",
    "print(f'binarized matix:')\n",
    "print(data_binarized)"
   ]
  },
  {
   "source": [
    "- 二值化之后的数据点都是0或者1，所以叫做二值化。\n",
    "- 算方法是，将所有大于threshold的数据都改为1，小于等于threshold的都设为0。\n",
    "- 经常用于出现某种特征（比如设为1），或者没有出现某种特征（设为0）的应用场合。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 独热编码(One-Hot Encoding)\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "one-hot encoded matrix:\n(1, 11)\n[[0. 0. 1. 0. 1. 0. 0. 0. 1. 1. 0.]]\n"
    }
   ],
   "source": [
    "import numpy as np \n",
    "from sklearn import preprocessing\n",
    "\n",
    "data=np.array([[0,2,1,12],\n",
    "               [1,3,5,3],\n",
    "               [2,3,2,12],\n",
    "               [1,2,4,3]]) # 原始数据矩阵 shape=(4,4)\n",
    "\n",
    "\n",
    "encoder = preprocessing.OneHotEncoder()\n",
    "encoder.fit(data)\n",
    "encoded_vector = encoder.transform([[2,3,5,3]]).toarray()\n",
    "\n",
    "print(f'one-hot encoded matrix:')\n",
    "print(encoded_vector.shape)\n",
    "print(encoded_vector)"
   ]
  },
  {
   "source": [
    "- 独热编码可以缩小特征向量的维度，将稀疏的、散乱的数据集（比如代码块中的data，shape=(4,4)）收缩为11维致密矩阵（如输出结果，shape=(1,11)）。\n",
    "- 编码方式为：根据原始数据集data构建编码器encoder，用编码器来对新数据进行编码。比如，第0列有三个不同值（0,1,2），故而有三个维度，即0=100，1=010，2=001；同理，第1列有两个不同值（2,3），故而只有两个维度，即2=10，3=01；同理，第2列有四个不同值（1,5,2,4），故而有四个维度，即1=1000，2=0100,4=0010,5=0001同理，第3列有两个不同值（3,12），故而只有两个维度，即3=10，12=01。所以在面对新数据[[2,3,5,3]]时，第0列的2就对应于001，第二列的3对应于01，第三列的5对应于0001，第四列的3对应于10，连接起来后就是输出的这个（1,11）矩阵，即为读了编码后的致密矩阵。\n",
    "- 如果面对的新数据不存在上面的编码器中，比如[[2,3,5,4]]时，4不存在于第3列（只有两个离散值3和12），则输出为00，连接起来后是[[0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 0.]]，注意倒数第二个数字变成了0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}