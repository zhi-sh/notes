{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(2021)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 点积缩放注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q,k,v = [bs, seq, feats]\n",
    "        d_k = k.size(-1)\n",
    "        assert d_k == q.size(-1)\n",
    "\n",
    "        # 1. calculate score\n",
    "        k = k.transpose(-1, -2)  # [ bs, feats, seq]\n",
    "        scores = torch.bmm(q, k)  # [bs, seq, seq]\n",
    "\n",
    "        # 2. divide by sqrt(d_k)\n",
    "        scores = scores / math.sqrt(d_k)\n",
    "\n",
    "        # 3. mask optional\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask, 0)\n",
    "\n",
    "        # 4. softmax\n",
    "        scores = torch.exp(scores)\n",
    "        scores = scores / scores.sum(dim=-1, keepdim=True)  # [bs, seq, seq]\n",
    "\n",
    "        scores = self.dropout(scores)\n",
    "\n",
    "        # 5. matmul with value matrix\n",
    "        context = torch.bmm(scores, v)  # [bs, seq, feats]\n",
    "\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaledDotProductAttention parameters: 0 \t size: torch.Size([5, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "attn = ScaledDotProductAttention()\n",
    "q = torch.rand(5, 10, 20)\n",
    "k = torch.rand(5, 10, 20)\n",
    "v = torch.rand(5, 10, 20)\n",
    "result = attn(q, k, v)\n",
    "print(f\"ScaledDotProductAttention parameters: {sum(x.numel() for x in attn.parameters())} \\t size: {result.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, dropout=0.1):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.attn = ScaledDotProductAttention(dropout)\n",
    "        self.tfm_query = nn.Linear(d_model, d_feature)\n",
    "        self.tfm_key = nn.Linear(d_model, d_feature)\n",
    "        self.tfm_value = nn.Linear(d_model, d_feature)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        Q = self.tfm_query(queries)  # [bs, seq, feats]\n",
    "        K = self.tfm_key(keys)  # [bs, seq, feats]\n",
    "        V = self.tfm_value(values)  # [bs, seq, feats]\n",
    "\n",
    "        context = self.attn(Q, K, V, mask)\n",
    "        return context\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout=0.1):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_feature = d_feature\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        assert d_model == d_feature * n_heads\n",
    "\n",
    "        self.attn_heads = nn.ModuleList([\n",
    "            AttentionHead(d_model=d_model, d_feature=d_feature, dropout=dropout) for _ in range(n_heads)\n",
    "        ])\n",
    "        self.projection = nn.Linear(n_heads * d_feature, d_model)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        xs = [attn(queries, keys, values, mask=mask) for i, attn in enumerate(self.attn_heads)]  # n_heads * [bs, seq, feats]\n",
    "\n",
    "        # 拼接\n",
    "        xs = torch.cat(xs, dim=-1)  # [bs, seq, feats * n_heads(=d_model)]\n",
    "\n",
    "        # 将多个ATTN输出结果映射\n",
    "        out = self.projection(xs)  # [bs, seq, d_model]\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AttentionHead parameters: 1260 \t size: torch.Size([5, 10, 20])\n",
      "MultiHeadAttention parameters: 103040 \t size: torch.Size([5, 10, 160])\n"
     ]
    }
   ],
   "source": [
    "attn_head = AttentionHead(20, 20)\n",
    "result = attn_head(q, k, v)\n",
    "print(f\"AttentionHead parameters: {sum(x.numel() for x in attn_head.parameters())} \\t size: {result.size()}\")\n",
    "\n",
    "d_model = 20 * 8\n",
    "d_feature = 20\n",
    "n_heads = 8\n",
    "\n",
    "heads = MultiHeadAttention(d_model=d_model, d_feature=d_feature, n_heads=n_heads)\n",
    "result = heads(q.repeat(1, 1, 8), k.repeat(1, 1, 8), v.repeat(1, 1, 8))\n",
    "print(f\"MultiHeadAttention parameters: {sum(x.numel() for x in heads.parameters())} \\t size: {result.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-8):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.eps) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder子层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        print\n",
    "        self.attn_head = MultiHeadAttention(d_model=d_model, d_feature=d_feature, n_heads=n_heads, dropout=dropout)\n",
    "        self.layer_norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(d_model=d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1.1 multi head attention\n",
    "        att = self.attn_head(x, x, x, mask=mask)\n",
    "        # 1.2 apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "\n",
    "        # 2.1 apply position-wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        # 2.2 apply normalization and residual connection\n",
    "        x = x + self.dropout(self.layer_norm2(pos))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderBlock parameters: 3152384 \t size: torch.Size([5, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "enc = EncoderBlock()\n",
    "result = enc(torch.rand(5, 10, 512))\n",
    "print(f\"EncoderBlock parameters: {sum(x.numel() for x in enc.parameters())} \\t size: {result.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, n_blocks=6, d_model=512, n_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, n_heads=n_heads, dropout=dropout) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, mask=None):\n",
    "        for encoder in self.encoders:\n",
    "            x = encoder(x, mask=mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerEncoder parameters: 18914304 \t size: torch.Size([5, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "t_enc = TransformerEncoder()\n",
    "result = t_enc(torch.rand(5, 10, 512))\n",
    "print(f\"TransformerEncoder parameters: {sum(x.numel() for x in t_enc.parameters())} \\t size: {result.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder子层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model=512, d_feature=64, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model=d_model, d_feature=d_feature, n_heads=n_heads, dropout=dropout)\n",
    "        self.attn_head = MultiHeadAttention(d_model=d_model, d_feature=d_feature, n_heads=n_heads, dropout=dropout)\n",
    "        self.position_wise_feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.layer_norm1 = LayerNorm(d_model=d_model)\n",
    "        self.layer_norm2 = LayerNorm(d_model=d_model)\n",
    "        self.layer_norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_self_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        # 1.1 apply masked multihead attention\n",
    "        att = self.masked_attn_head(x, x, x, mask=dec_self_attn_mask)\n",
    "        # 1.2 layer normalization\n",
    "        x = x + self.dropout(self.layer_norm1(att))\n",
    "\n",
    "        # 2.1 apply multihead attention (encoder and decoder)\n",
    "        att = self.attn_head(queries=x, keys=enc_out, values=enc_out, mask=dec_enc_attn_mask)\n",
    "        # 2.2 layer normalization\n",
    "        x = x + self.dropout(self.layer_norm2(att))\n",
    "\n",
    "        # 3.1 apply position wise feedforward network\n",
    "        pos = self.position_wise_feed_forward(x)\n",
    "        # 3.2 layer normalization\n",
    "        x = x + self.dropout(self.layer_norm3(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecoderBlock parameters: 4204032 \t size: torch.Size([5, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "t_enc = TransformerEncoder()\n",
    "dec = DecoderBlock()\n",
    "\n",
    "x = torch.rand(5, 10, 512)\n",
    "enc_out = t_enc(torch.rand(5, 10, 512))\n",
    "result = dec(x, enc_out)\n",
    "print(f\"DecoderBlock parameters: {sum(x.numel() for x in dec.parameters())} \\t size: {result.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, n_blocks=6, d_model=512, d_ff=2048, n_heads=8, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderBlock(d_model=d_model, d_feature=d_model // n_heads, d_ff=d_ff, n_heads=n_heads, dropout=dropout) for _ in range(n_blocks)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x: torch.FloatTensor, enc_out: torch.FloatTensor, dec_self_attn_mask=None, dec_enc_attn_mask=None):\n",
    "        for decoder in self.decoders:\n",
    "            x = decoder(x, enc_out, dec_self_attn_mask, dec_enc_attn_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerDecoder parameters: 25224192 \t size: torch.Size([5, 10, 512])\n"
     ]
    }
   ],
   "source": [
    "t_enc = TransformerEncoder()\n",
    "t_dec = TransformerDecoder()\n",
    "\n",
    "x = torch.rand(5, 10, 512)\n",
    "enc_out = t_enc(torch.rand(5, 10, 512))\n",
    "result = t_dec(x, enc_out)\n",
    "print(f\"TransformerDecoder parameters: {sum(x.numel() for x in t_dec.parameters())} \\t size: {result.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词向量编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 位置编码\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=512):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.weight = nn.Parameter(pe, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.weight[:, :x.size(1), :]  # [1, seq, feat)\n",
    "\n",
    "\n",
    "# 词向量编码\n",
    "class WordPositionEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512):\n",
    "        super(WordPositionEmbedding, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_embedding = PositionalEmbedding(d_model)\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.FloatTensor:\n",
    "        return self.word_embedding(x) + self.positional_embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer parameters: 44138496 \t size: torch.Size([5, 30, 512])\n"
     ]
    }
   ],
   "source": [
    "emb = WordPositionEmbedding(vocab_size=1000)\n",
    "encoder = TransformerEncoder()\n",
    "decoder = TransformerDecoder()\n",
    "\n",
    "src_ids = torch.randint(1000, (5, 30))\n",
    "tgt_ids = torch.randint(1000, (5, 30))\n",
    "x = encoder(emb(src_ids))\n",
    "result = decoder(emb(tgt_ids), x)\n",
    "print(f\"Transformer parameters: {sum(x.numel() for x in list(encoder.parameters()) + list(decoder.parameters()))} \\t size: {result.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
